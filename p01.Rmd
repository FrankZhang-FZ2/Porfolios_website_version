---
title: "Portfolio 1"
author: "Qilin Zhang"
date: "2023-01-24"
output: html_document
---

##text preparation and wordcloud generation
This example contains my efforts to analyze the text in the book named Stranger Drowning. I was trying to get an idea of what word appears most frequently in the book and use it as a guide to a scale development project. Here most of what I did is to first convert the pdf book into text data. Then, I prepared the text for analysis (e.g. removing stop words, punctuation, etc.). At the end, I got a frequency count of the word and put them into a wordcloud. 

[citation](https://www.r-bloggers.com/2021/02/text-analysis-with-r/)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
txt <- read.delim("stranger drowning.txt", header = FALSE)
```

##set up

```{r packs, message=FALSE}
library(tidytext)
library(tidyverse)
library(textdata)
library(wordcloud)
```

##text_analyses

```{r analyses}

txt_tidy <- txt %>%
  unnest_tokens("Word", "V1") %>%
  mutate(Word = str_replace(Word, "'s", ""))%>%
  anti_join(stop_words, by = c("Word" = "word"))

txt_tidy_S <- txt %>%
  unnest_tokens("Word", "V1") %>%
  mutate(Word = str_replace(Word, "'s", ""))%>%
  anti_join(stop_words, by = c("Word" = "word")) %>%
  inner_join(get_sentiments("nrc"), by = c("Word" = "word"))

word_frequency <- function(x, top = 10){
  
  x %>%
    
# generate the word count
  count(Word, sort = TRUE) %>%
  
# We want to create a factor from the word column with the levels showing the most frequent words as top level
# This is just for aestethic reasons, however, it helps make the point
  mutate(Word = factor(Word, levels = rev(unique(Word)))) %>% 
# We use the "top" variable defined in the function so we can decide how many words we want to use 
  top_n(top) %>%
    
# This will be useful later if we want to use a grouping variable and will do nothing if we don't  
  ungroup() %>%
  
# The graph itself
  ggplot(mapping = aes(x = Word, y = n)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL)
}

txt_tidy %>%
word_frequency(30)

txt_tidy_S %>%
  count(sentiment, sort = TRUE) %>%
  ggplot(mapping = aes(x = sentiment, y = n)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL)
```

##wordcloud generation

```{r Wordcloud}
txt_tidy %>%
  count(Word) %>%
  with(wordcloud(Word, n, max.words = 100))
```