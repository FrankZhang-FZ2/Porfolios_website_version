---
title: "Portfolios 6"
author: "Qilin Zhang"
date: "2023-04-20"
output: html_document
---
##PolytomousIRT(General Partial Credit Model)

In this example, I employed Generalized Partial Credit Model (GPCM) to analyze the data I gathered, aiming to validate the psychometric properties of a newly developed scale. The goal is to design item that is highly discriminating for people with high latent trait (i.e. extreme altruism). The item probablity and information graph provides insight into this aspect. 

You can refer to this paper if you are interested in GPCM
Muraki, E. (1992). A Generalized Partial Credit Model: Application of an EM Algorithm. Applied Psychological Measurement, 16(2), 159â€“176.


``````{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)

```

```{r pac, message = FALSE, include=FALSE}
library(summarytools)
library(tidyverse) #data wrangling
library(codebook) #codebook generation
library(future) #reliability
library(ufs) #reliability
library(GGally) #reliability
library(GPArotation) #reliability
library(rio) #reading in different file types
library(labelled) #labeling data
library(psych)
library(corrplot) 
library("psych")
library(mirt)
library(eRm)
library(mice)
library(lavaan)
library(semPlot)
library(parameters)
library(broom)
library(likert)
library(sjPlot)

#if need to download the epmr package
#if (!require("devtools")) install.packages("devtools")   
#devtools::install_github("talbano/epmr")
```

###cleaning

```{r cleaning code, include = FALSE}

## use other read functions as appropriate for file type

dict <- rio::import(file = "Data/DGS_S1_dictionary.xlsx") #dictionary

data <- read.csv(file = 'Data/DGS_S1_Deidentified For Analyses.csv', sep = ",") #data

##trim based on completion time (>180)
data <- data %>%
  filter(Duration..in.seconds. >= 180)

data <- data[-c(1:2),-c(1:7)]

## Variable types 
names <- dict %>% 
  filter(type == "character") %>% 
  pull(variable)
data[,names] <- 
  lapply(data[,names], as.character)

names <- dict %>% 
  filter(type == "factor") %>% 
  pull(variable)
data[,names] <- 
  lapply(data[,names], as.numeric) #factor variables are coded as numeric for codebook purposes

names <- dict %>% 
  filter(type == "numeric") %>% 
  pull(variable)
data[,names] <- 
  lapply(data[,names], as.numeric)

rm(names)

##data completion check and imputation

#remove failed attention check -55 participants from this process
data <- data %>%
  filter(DGS_31 == 4) %>%
  filter((DGS_53 == 2))

#prepare dataframe for different scales
DGS <- data[grep("DGS_1",colnames(data)):grep("DGS_75",colnames(data))] %>%
  select(!c(DGS_31,DGS_53))


##remove people that gave 90% of the same answer in DGS

for(i in 1:7){
  percent <- function(x){
  sum((x == i)/length(x)*100)
  }
  number <- apply(DGS,1,percent)
  data <- data[c(number<90),]
}

#function for checking percentage of missing data (unit=%)
percent_missing <- function(x){
  sum(is.na((x))/length(x)*100)
}
missing_R <- apply(data,1,percent_missing)
table(missing_R)

#use if don't want imputation(turn off if I need imputation)
replace_rows <- subset(data, missing_R<=0)
data <- replace_rows
```

```{r imputation, include= FALSE}
if(FALSE){
#subset based on filtering criteria
replace_rows <- subset(data, missing_R<=10)
no_rows <- subset(data, missing_R>10)

missing_C <- apply(replace_rows,2,percent_missing)
table(missing_C) #no concern here

replace_data <- replace_rows[,1:85]
leftout <- replace_rows[,86:91]

#check where the NAs are if needed
rindex <- rep(FALSE, nrow(replace_data))
for (i in 1:nrow(replace_data)){
  for (j in 1:grep("DGS_75",colnames(replace_data))){
    if( is.na(replace_data[i,j])){
      rindex[i] = TRUE
      j = ncol(replace_data)+1
    }
  }
}
data_error <- replace_data[rindex,]

rm(data_error)
#imputation
temp <- mice(replace_data)
fixed_data <- complete(temp)  #imputation using mice package
data <- cbind(fixed_data,leftout) #no additional participants were removed

rm(fixed_data,leftout,no_rows,replace_data,replace_rows)
}
```

```{r recode, include=FALSE}
##recode

#DGS recode
likert <- dict %>% 
  filter (value_label == "1 = Strongly disagree, 2 = Disagree, 3 = Somewhat disagree, 4 = Neither agree nor disagree, 5 = Somewhat agree, 6 = Agree, 7 = Strongly agree") %>%
  pull(variable)
add_likert <- function(x) {
  val_labels(x) <- c("Strongly disagree"= 1, "Disagree" = 2, "Somewhat disagree" = 3, "Neither agree nor disagree" = 4, "Somewhat agree" = 5, "Agree" = 6, "Strongly agree" = 7) 
  x
}
data <- data %>%
  mutate_at(likert, 
            add_likert)  

rm(likert, add_likert)

#HEXACO_C

likert <- dict %>% 
  filter (value_label == "1 = Strongly disagree, 2 = Somewhat disagree, 3 = Neither agree nor disagree, 4 = Somewhat agree, 5 = Strongly agree") %>%
  pull(variable)
add_likert <- function(x) {
  val_labels(x) <- c("Strongly disagree"= 1, "Somewhat disagree" = 2, "Neither agree nor disagree" = 3, "Somewhat agree" = 4, "Strongly agree" = 5) 
  x
}
data <- data %>%
  mutate_at(likert, 
            add_likert)  

rm(likert, add_likert)

## Reverse-scoring 
reversed_items <- dict %>%  #make a list of reversed items
 filter (keying == -1) %>% 
 pull(variable)

data <- data %>%  #reverse values in data
  mutate_at(reversed_items,
          reverse_labelled_values)

rm(reversed_items)

##scale construction

## Variable labels
var_label(data) <- dict %>% 
  select(variable, label) %>% 
  dict_to_list()

rm(extra)

##DGS
DGS <- dict %>% 
  filter (scale == "DGS") %>% 
  pull(variable)

data$DGS <- data %>% 
  select(all_of(DGS)) %>% 
  aggregate_and_document_scale()

##HEXACO_C

HEXACO_C <- dict %>% 
  filter (scale == "HEXACO_C") %>% 
  pull(variable)

data$HEXACO_C <- data %>% 
  select(all_of(HEXACO_C)) %>% 
  aggregate_and_document_scale()

###7 factors model
M7_1 <- dict %>% 
  filter (M7_concise == 1) %>% 
  pull(variable)
M7_2 <- dict %>% 
  filter (M7_concise == 2) %>% 
  pull(variable)
M7_3 <- dict %>% 
  filter (M7_concise == 3) %>% 
  pull(variable)
M7_4 <- dict %>% 
  filter (M7_concise == 4) %>% 
  pull(variable)
M7_5 <- dict %>% 
  filter (M7_concise == 5) %>% 
  pull(variable)
M7_6 <- dict %>% 
  filter (M7_concise == 6) %>% 
  pull(variable)
M7_7 <- dict %>% 
  filter (M7_concise == 7) %>% 
  pull(variable)

###7 factors model

data$M7_1 <- data %>% 
  select(all_of(M7_1)) %>% 
  aggregate_and_document_scale()

data$M7_2 <- data %>% 
  select(all_of(M7_2)) %>% 
  aggregate_and_document_scale()

data$M7_3 <- data %>% 
  select(all_of(M7_3)) %>% 
  aggregate_and_document_scale()

data$M7_4 <- data %>% 
  select(all_of(M7_4)) %>% 
  aggregate_and_document_scale()

data$M7_5 <- data %>% 
  select(all_of(M7_5)) %>% 
  aggregate_and_document_scale()

data$M7_6 <- data %>% 
  select(all_of(M7_6)) %>% 
  aggregate_and_document_scale()

data$M7_7 <- data %>% 
  select(all_of(M7_7)) %>% 
  aggregate_and_document_scale()
```

```{r preparation, include= FALSE}

##rename
data_cleaned <- data

#prepare dataframe for different scales
DGS <- data_cleaned[grep("DGS_1",colnames(data_cleaned)):grep("DGS_75",colnames(data_cleaned))]
DGS <- DGS[,-c(grep("DGS_31",colnames(DGS)),grep("DGS_53",colnames(DGS)))]

HEXACO_C <- data_cleaned[grep("HEXACO_C_1",colnames(data_cleaned)):grep("HEXACO_C_10",colnames(data_cleaned))]
```

```{r assign}
#6 factors model without behavioral items
DGS_6model_1 <- DGS %>%
  select(c(DGS_4,DGS_12,DGS_15,DGS_17,DGS_18,DGS_19,DGS_22,DGS_30,DGS_32,DGS_33,DGS_34,DGS_36,DGS_56,DGS_49,DGS_57,DGS_58,DGS_60,DGS_61,DGS_63,DGS_74,DGS_75))

```
#descriptive of the scale

```{r Likert}
visu_likert <- function(x,title){
  x <- as.data.frame(lapply(x,factor))
  likert_x <- likert(x)
  likert_fit_x <- likert.bar.plot(likert_x)
  plot(likert_fit_x)
}

visu_likert(DGS_6model_1)#Prosocial value
```

###IRT(I only included an example here with one latent variable)

## (dropping rule is loading in IFA < 0.3)

```{R IRT6_1}

  temp <- capture.output({
    DGS_IRT6_1 <- mirt(data = DGS_6model_1,
      model = 1, #this is for one factor
      itemtype = "gpcmIRT" #generalized partial credit model
)
    })
```

```{R}
summary(DGS_IRT6_1)
plot(DGS_IRT6_1, type = "trace")
plot(DGS_IRT6_1, type = "info")
plot(DGS_IRT6_1) #expected score curve
```



## revision based on previous results

```{R revise}
##recommend taking out 4 and 22
DGS_6model_1 <- DGS_6model_1%>%
  select(!c(DGS_4,DGS_22))

  temp <- capture.output({
    DGS_IRT6_1 <- mirt(data = DGS_6model_1,
      model = 1, #this is for one factor
      itemtype = "gpcmIRT" #generalized partial credit model
)
    })
```
```{r}
summary(DGS_IRT6_1)
plot(DGS_IRT6_1, type = "trace")
#traces suggest that 2-4 options were not commonly used
plot(DGS_IRT6_1, type = "info")
#item information suggests that we need to improve the ability to identify people on top of the altruism scores. 


#person fit
#mirt::itemfit(DGS_IRT6_1)
#DGS_IRT6_1_person_fit <- mirt::personfit(DGS_IRT6_1)
```
