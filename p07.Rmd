---
title: "Portfolios 7"
author: "Qilin Zhang"
date: "2023-04-20"
output: html_document
---

###Web scarping
In this example, I scarped from the University of Edinburgh Museum of Art website. I pulled information about their art pieces from over 90 pages of information. At the end, I included a very simple visualization for the age of the collection. 

This exercise is derived from a exercise I did in lab 08 in [Data Science for Psychologists](https://datascience4psych.github.io/DataScience4Psych/lab08.html).


```{r load-packages, message = FALSE}

library(tidyverse)
library(tibble)
library(skimr)
library(rvest)

```

```{r load-data, message = FALSE}

first_url <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0"
page <- read_html(first_url)

titles <- page %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>%
  str_squish()

```

#Make a quick trial to make sure the code works

```{r fix_links}

Links <- page %>%
  html_nodes(".iteminfo") %>%   # same nodes
  html_node("h3 a") %>%         # as before
  html_attr("href") %>%
  str_replace("./","https://collections.ed.ac.uk/art/")

```

```{r artist_names}

Artists_names <- page %>%
  html_nodes(".artist") %>%
  html_text() %>%
  str_squish()

```

```{r tibbles}
#everything <- tibble(Artists_names= Artists_names,titles = titles, links = Links)

  
max_length <- max(length(Artists_names), length(titles), length(Links))
Artists_names <- c(Artists_names, rep(NA, max_length - length(Artists_names)))
titles <- c(titles, rep(NA, max_length - length(titles)))
Links <- c(Links, rep(NA, max_length - length(Links)))

everything <- tibble("Artists_names"= Artists_names,"titles" = titles, "links" = Links)
  
```


#function setup

```{r scrape_function}
scrape_page <- function(x){
  y <- read_html(x)
  #titles
  titles <- y %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>%
  str_squish()
  #Links
  Links <- y %>%
  html_nodes(".iteminfo") %>% 
  html_node("h3 a") %>%         
  html_attr("href") %>%
  str_replace("./","https://collections.ed.ac.uk/art/")
  #Artists_names
  Artists_names <- y %>%
  html_nodes(".artist") %>%
  html_text() %>%
  str_squish()
  #combine
  max_length <- max(length(Artists_names), length(titles), length(Links))
  Artists_names <- c(Artists_names, rep(NA, max_length - length(Artists_names)))
  titles <- c(titles, rep(NA, max_length - length(titles)))
  Links <- c(Links, rep(NA, max_length - length(Links)))
  df <- tibble("Artists_names"= Artists_names,"titles" = titles, "links" = Links)
  return(df)
}

scrape_page(first_url)

rm(page,titles,Links,Artists_names)
```

#iterative scraping

```{r URL_list}
URL_list <- list("URL"=
                     paste("https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=",
                           seq(0,2900,by = 10),
                           sep = ""
                           )
                   )
URL_list <- unlist(URL_list)

# Let's scrape!
uoe_art<-map_dfr(URL_list,scrape_page)

```


#save it here so that I don't need to run it again

```{r separate-title-date, error = TRUE}
write.csv(uoe_art, "Data/uoe_art.csv")
```

#prepare data and seperate date

```{r seperate_date}
uoe_art <- read_csv("Data/uoe_art.csv")
uoe_art <- uoe_art %>%
  separate(titles, into = c("titles", "date"), sep = "\\(") %>%
  mutate(year = str_remove(date, "\\)") %>% as.numeric()) %>%
  select(c(Artists_names,titles,links,year))

```


#Quick skim

```{r skim}

skim(uoe_art)

uoe_art %>%
  select(year)%>%
  summary
```

#simple visualization

```{r hist, warning=FALSE}

uoe_art %>%
  ggplot(aes(x=year))+
  geom_density()+
  xlim(1900,2023)
#there is a piece that came from year 2 which is not included in the graph. 
```
